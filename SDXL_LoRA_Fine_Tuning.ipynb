{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOI35pBKaZgOrzHd0yaXMMk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidricardocr/sdxl-lora-fine-tuning/blob/main/SDXL_LoRA_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stable Diffusion XL LoRA Fine-Tuning Guide"
      ],
      "metadata": {
        "id": "ISIFDlyXddJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this guide on fine-tuning **Stable Diffusion XL (SDXL) with LoRA (Low-Rank Adaptation)**. In this notebook, we will walk through setting up the environment, executing the fine-tuning script, and loading the resulting weights for inference.\n",
        "\n",
        "**Objective**: Our goal is to customize the SDXL model to generate images with specific styles or themes. To achieve this, we use LoRA, a parameter-efficient fine-tuning technique, making the process computationally feasible even on limited hardware.\n",
        "\n",
        "In this tutorial, we will fine-tune the model using a **Pokémon-themed dataset**, `svjack/pokemon-blip-captions-en-zh`, which consists of a collection of Pokémon images paired with captions in both English and Chinese. This dataset offers a rich variety of images and descriptions that provide a creative and structured source for training the model. By using this dataset, we aim to adapt Stable Diffusion XL to generate unique, Pokémon-inspired visuals aligned with the captions' themes.\n",
        "\n",
        "### Getting Started\n",
        "\n",
        "We will start by downloading the setup files (`build.sh` and `config.yaml`) directly from the GitHub repository. These files will help us configure the environment and set the necessary parameters for fine-tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "zpGKdS0xgP3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the build.sh script\n",
        "!wget https://github.com/davidricardocr/sdxl-lora-fine-tuning/raw/main/build.sh -O build.sh\n",
        "\n",
        "# Download the config.yaml file\n",
        "!wget https://github.com/davidricardocr/sdxl-lora-fine-tuning/raw/main/config.yaml -O config.yaml"
      ],
      "metadata": {
        "id": "xiQ_4x74v9Ro",
        "outputId": "2eb0a886-30cf-4df4-a571-9391c25e7876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-03 15:59:02--  https://github.com/davidricardocr/sdxl-lora-fine-tuning/raw/main/build.sh\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/davidricardocr/sdxl-lora-fine-tuning/main/build.sh [following]\n",
            "--2024-11-03 15:59:03--  https://raw.githubusercontent.com/davidricardocr/sdxl-lora-fine-tuning/main/build.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1007 [text/plain]\n",
            "Saving to: ‘build.sh’\n",
            "\n",
            "build.sh            100%[===================>]    1007  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-03 15:59:03 (100 MB/s) - ‘build.sh’ saved [1007/1007]\n",
            "\n",
            "--2024-11-03 15:59:03--  https://github.com/davidricardocr/sdxl-lora-fine-tuning/raw/main/config.yaml\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/davidricardocr/sdxl-lora-fine-tuning/main/config.yaml [following]\n",
            "--2024-11-03 15:59:03--  https://raw.githubusercontent.com/davidricardocr/sdxl-lora-fine-tuning/main/config.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 254 [text/plain]\n",
            "Saving to: ‘config.yaml’\n",
            "\n",
            "config.yaml         100%[===================>]     254  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-03 15:59:03 (29.1 MB/s) - ‘config.yaml’ saved [254/254]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "To simplify the environment setup, we have created a shell script (build.sh) that installs the necessary dependencies for SDXL fine-tuning. This includes cloning the diffusers repository, installing specific requirements for SDXL examples, and configuring tools to optimize computation.\n",
        "\n",
        "Simply run the following cell to execute the setup:"
      ],
      "metadata": {
        "id": "7lN7R0cYg6jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the setup script\n",
        "!bash build.sh"
      ],
      "metadata": {
        "id": "hey2MIxgddw0",
        "outputId": "0d8a9e84-b286-46ff-e1e5-afd961741314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning Hugging Face diffusers repository...\n",
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 73564, done.\u001b[K\n",
            "remote: Counting objects: 100% (13000/13000), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1297/1297), done.\u001b[K\n",
            "remote: Total 73564 (delta 12433), reused 11880 (delta 11632), pack-reused 60564 (from 1)\u001b[K\n",
            "Receiving objects: 100% (73564/73564), 51.46 MiB | 31.37 MiB/s, done.\n",
            "Resolving deltas: 100% (54669/54669), done.\n",
            "Installing diffusers in editable mode...\n",
            "Obtaining file:///content/diffusers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.32.0.dev0) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.32.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.32.0.dev0) (0.24.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.32.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.32.0.dev0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.32.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.32.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.32.0.dev0) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.32.0.dev0) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.32.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.32.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.32.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.32.0.dev0) (2024.8.30)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building editable for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.32.0.dev0-0.editable-py3-none-any.whl size=11113 sha256=1ffdb02ce2452358aad0fa28c00df67c4eda71bc93430212bb418c3cc222bc4e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-svfuys_g/wheels/95/c5/3b/e1b4269f8a2584de57e75f949a185b48fc4144e9a91fc9965a\n",
            "Successfully built diffusers\n",
            "Installing collected packages: diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.30.3\n",
            "    Uninstalling diffusers-0.30.3:\n",
            "      Successfully uninstalled diffusers-0.30.3\n",
            "Successfully installed diffusers-0.32.0.dev0\n",
            "Installing additional requirements for SDXL examples...\n",
            "Requirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements_sdxl.txt (line 1)) (0.34.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements_sdxl.txt (line 2)) (0.20.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements_sdxl.txt (line 3)) (4.44.2)\n",
            "Collecting ftfy (from -r requirements_sdxl.txt (line 4))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements_sdxl.txt (line 5)) (2.17.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from -r requirements_sdxl.txt (line 6)) (3.1.4)\n",
            "Collecting datasets (from -r requirements_sdxl.txt (line 7))\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting peft==0.7.0 (from -r requirements_sdxl.txt (line 8))\n",
            "  Downloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements_sdxl.txt (line 8)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements_sdxl.txt (line 8)) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements_sdxl.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements_sdxl.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements_sdxl.txt (line 8)) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements_sdxl.txt (line 8)) (4.66.6)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements_sdxl.txt (line 8)) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements_sdxl.txt (line 8)) (0.24.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements_sdxl.txt (line 2)) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements_sdxl.txt (line 8)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements_sdxl.txt (line 8)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements_sdxl.txt (line 8)) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements_sdxl.txt (line 8)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements_sdxl.txt (line 8)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.0->-r requirements_sdxl.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements_sdxl.txt (line 3)) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements_sdxl.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements_sdxl.txt (line 3)) (0.19.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements_sdxl.txt (line 4)) (0.2.13)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements_sdxl.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements_sdxl.txt (line 5)) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements_sdxl.txt (line 5)) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements_sdxl.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements_sdxl.txt (line 5)) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements_sdxl.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements_sdxl.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements_sdxl.txt (line 5)) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->-r requirements_sdxl.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements_sdxl.txt (line 7)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements_sdxl.txt (line 7))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements_sdxl.txt (line 7)) (2.2.2)\n",
            "Collecting xxhash (from datasets->-r requirements_sdxl.txt (line 7))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->-r requirements_sdxl.txt (line 7))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch>=1.13.0->peft==0.7.0->-r requirements_sdxl.txt (line 8))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements_sdxl.txt (line 7)) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 7)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 7)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 7)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 7)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements_sdxl.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements_sdxl.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements_sdxl.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements_sdxl.txt (line 3)) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements_sdxl.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements_sdxl.txt (line 7)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements_sdxl.txt (line 7)) (2024.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->-r requirements_sdxl.txt (line 7)) (0.2.0)\n",
            "Downloading peft-0.7.0-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, ftfy, fsspec, dill, multiprocess, peft, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.13.2\n",
            "    Uninstalling peft-0.13.2:\n",
            "      Successfully uninstalled peft-0.13.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 ftfy-6.3.1 multiprocess-0.70.16 peft-0.7.0 xxhash-3.5.0\n",
            "Installing bitsandbytes for optimized computation...\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.44.1\n",
            "Adding /content/diffusers to PYTHONPATH...\n",
            "Setup complete. Your environment is ready to run LoRA fine-tuning scripts with diffusers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning with LoRA and `accelerate`\n",
        "\n",
        "Now that the environment is ready, let's fine-tune SDXL using LoRA. The following command leverages the `accelerate` library to handle efficient parallelism and optimization for large model training. Here’s a breakdown of each parameter:\n",
        "\n",
        "- `--pretrained_model_name_or_path`: The base model to fine-tune. Here, we're using `stabilityai/stable-diffusion-xl-base-1.0`.\n",
        "- `--pretrained_vae_model_name_or_path`: The pre-trained VAE model path for stable generation. We use `sdxl-vae-fp16-fix` for improved image quality.\n",
        "- `--dataset_name`: Specifies the dataset to use; in this case, a Pokémon captioning dataset.\n",
        "- `--dataloader_num_workers`: Set to 8 to increase data loading efficiency during training.\n",
        "- `--caption_column`: The column in the dataset that provides the captions for image generation.\n",
        "- `--train_batch_size`: Batch size of 5 helps balance memory load and training speed.\n",
        "- `--num_train_epochs`: We set this to 10, allowing sufficient training while controlling computational time.\n",
        "- `--learning_rate`: A low rate (1e-4) to prevent overfitting during fine-tuning.\n",
        "- `--lr_scheduler`: \"constant\" maintains a steady learning rate, simplifying training stability.\n",
        "- `--resolution`: Output image resolution. Here, we use 512 for quality and computational feasibility.\n",
        "- `--center_crop` & `--random_flip`: Basic data augmentations to improve model robustness.\n",
        "- `--output_dir`: Where fine-tuned weights will be saved.\n",
        "- `--validation_prompt`: Used to periodically check the model's output during training.\n",
        "- `--checkpointing_steps`: Save model checkpoints every 500 steps.\n",
        "- `--gradient_checkpointing` & `--gradient_accumulation_steps`: Techniques to handle large model gradients without overwhelming memory.\n",
        "- `--mixed_precision=\"fp16\"`: 16-bit floating point precision to reduce memory usage.\n",
        "- `--use_8bit_adam`: Optimizes computation with 8-bit Adam optimizer, reducing resource needs.\n",
        "- `--seed`: Setting for reproducibility.\n",
        "\n",
        "Run the following cell to start fine-tuning:\n"
      ],
      "metadata": {
        "id": "AbyvhH428IRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --config_file config.yaml diffusers/examples/text_to_image/train_text_to_image_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
        "  --dataset_name=\"svjack/pokemon-blip-captions-en-zh\" \\\n",
        "  --dataloader_num_workers=8 \\\n",
        "  --caption_column=\"en_text\" \\\n",
        "  --train_batch_size=5 \\\n",
        "  --num_train_epochs=10 \\\n",
        "  --learning_rate=1e-04 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --resolution=512 \\\n",
        "  --center_crop \\\n",
        "  --random_flip \\\n",
        "  --output_dir=\"sdxl-lora-weights\" \\\n",
        "  --validation_prompt=\"A cat in the forest.\" \\\n",
        "  --num_validation_images=5 \\\n",
        "  --checkpointing_steps=500 \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --seed=42\n"
      ],
      "metadata": {
        "id": "TizXnaTahUvx",
        "outputId": "5e9dab5b-0cae-493b-98a8-6ef5f12cd3dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-03 16:00:27.878071: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-03 16:00:27.893925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-03 16:00:27.914515: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-03 16:00:27.920875: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-03 16:00:27.936233: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-03 16:00:29.154832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "11/03/2024 16:00:30 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "tokenizer/tokenizer_config.json: 100% 737/737 [00:00<00:00, 6.43MB/s]\n",
            "tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 7.06MB/s]\n",
            "tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 3.71MB/s]\n",
            "tokenizer/special_tokens_map.json: 100% 472/472 [00:00<00:00, 5.29MB/s]\n",
            "tokenizer_2/tokenizer_config.json: 100% 725/725 [00:00<00:00, 6.60MB/s]\n",
            "tokenizer_2/special_tokens_map.json: 100% 460/460 [00:00<00:00, 3.97MB/s]\n",
            "text_encoder/config.json: 100% 565/565 [00:00<00:00, 3.55MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "text_encoder_2/config.json: 100% 575/575 [00:00<00:00, 6.40MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "scheduler/scheduler_config.json: 100% 479/479 [00:00<00:00, 4.69MB/s]\n",
            "{'clip_sample_range', 'dynamic_thresholding_ratio', 'thresholding', 'variance_type', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
            "model.safetensors: 100% 492M/492M [00:02<00:00, 227MB/s]\n",
            "model.safetensors: 100% 2.78G/2.78G [00:12<00:00, 223MB/s]\n",
            "config.json: 100% 631/631 [00:00<00:00, 5.84MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 237MB/s]\n",
            "{'mid_block_add_attention', 'latents_std', 'latents_mean', 'use_quant_conv', 'shift_factor', 'use_post_quant_conv'} was not found in config. Values will be initialized to default values.\n",
            "unet/config.json: 100% 1.68k/1.68k [00:00<00:00, 14.5MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 10.3G/10.3G [00:52<00:00, 197MB/s]\n",
            "{'attention_type', 'reverse_transformer_layers_per_block', 'dropout'} was not found in config. Values will be initialized to default values.\n",
            "README.md: 100% 1.14k/1.14k [00:00<00:00, 10.3MB/s]\n",
            "dataset_infos.json: 100% 804/804 [00:00<00:00, 8.49MB/s]\n",
            "(…)-00000-of-00001-78e564002aa9c8f0.parquet: 100% 99.7M/99.7M [00:00<00:00, 161MB/s]\n",
            "Generating train split: 100% 833/833 [00:00<00:00, 1453.65 examples/s]\n",
            "11/03/2024 16:01:57 - INFO - __main__ - ***** Running training *****\n",
            "11/03/2024 16:01:57 - INFO - __main__ -   Num examples = 833\n",
            "11/03/2024 16:01:57 - INFO - __main__ -   Num Epochs = 10\n",
            "11/03/2024 16:01:57 - INFO - __main__ -   Instantaneous batch size per device = 5\n",
            "11/03/2024 16:01:57 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 20\n",
            "11/03/2024 16:01:57 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
            "11/03/2024 16:01:57 - INFO - __main__ -   Total optimization steps = 420\n",
            "Steps:  10% 42/420 [04:14<39:35,  6.29s/it, lr=0.0001, step_loss=0.0708]\n",
            "model_index.json: 100% 609/609 [00:00<00:00, 4.84MB/s]\n",
            "\n",
            "Fetching 11 files:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   0% 0.00/335M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 21.0M/335M [00:00<00:02, 138MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 52.4M/335M [00:00<00:01, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 83.9M/335M [00:00<00:01, 226MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 115M/335M [00:00<00:00, 231MB/s] \u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 147M/335M [00:00<00:00, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 178M/335M [00:00<00:00, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  63% 210M/335M [00:00<00:00, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 241M/335M [00:01<00:00, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 273M/335M [00:01<00:00, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 304M/335M [00:01<00:00, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 226MB/s]\n",
            "\n",
            "Fetching 11 files: 100% 11/11 [00:01<00:00,  6.63it/s]\n",
            "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "\n",
            "Loading pipeline components...:  57% 4/7 [00:00<00:00, 27.36it/s]\u001b[A{'sigma_min', 'use_exponential_sigmas', 'final_sigmas_type', 'timestep_type', 'use_beta_sigmas', 'sigma_max', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 40.29it/s]\n",
            "11/03/2024 16:06:14 - INFO - __main__ - Running validation... \n",
            " Generating 5 images with prompt: A cat in the forest..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading LoRA Weights and Running Inference\n",
        "\n",
        "Once fine-tuning is complete, we can load the LoRA weights into the Stable Diffusion XL pipeline and run inference. The `diffusers` library provides a simple way to do this with the `StableDiffusionXLPipeline`, which allows us to leverage the fine-tuned model for custom image generation.\n",
        "\n",
        "The following code snippet loads the weights and generates an image based on a prompt. Two key parameters in this process are num_inference_steps and guidance_scale:\n",
        "\n",
        "* **num_inference_steps**: This parameter controls how many steps the model takes to generate the image. Higher values typically lead to more detailed images as the model has more iterations to refine the output. Here, we've set it to 100 to balance image quality with processing time.\n",
        "\n",
        "* **guidance_scale**: This parameter influences how closely the generated image follows the prompt. A higher guidance scale means the model will adhere more strictly to the prompt details, though excessively high values can sometimes affect image coherence. In this case, a guidance scale of 10 helps ensure the image aligns well with the prompt while maintaining visual quality.\n"
      ],
      "metadata": {
        "id": "L3rBoRAsiO-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "\n",
        "# Path where the LoRA weights are saved\n",
        "model_path = \"sdxl-lora-weights\"\n",
        "\n",
        "# Load the Stable Diffusion XL pipeline and set precision\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe.to(\"cuda\")  # Use GPU for faster inference\n",
        "\n",
        "# Load the fine-tuned LoRA weights\n",
        "pipe.load_lora_weights(model_path)\n",
        "\n",
        "# Generate an image with the fine-tuned model\n",
        "image = pipe(\n",
        "    prompt=\"A cat in the forest.\",\n",
        "    num_inference_steps=100,\n",
        "    guidance_scale=10\n",
        ").images[0]\n",
        "\n",
        "# Display the generated image\n",
        "image"
      ],
      "metadata": {
        "id": "wGiD7WoDiXf7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}