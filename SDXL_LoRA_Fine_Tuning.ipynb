{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOxW7dxDI/F/9JwoPub4deW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidricardocr/sdxl-lora-fine-tuning/blob/main/SDXL_LoRA_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stable Diffusion XL LoRA Fine-Tuning Guide"
      ],
      "metadata": {
        "id": "ISIFDlyXddJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this guide on fine-tuning Stable Diffusion XL (SDXL) with **LoRA (Low-Rank Adaptation)**. In this notebook, we will walk through setting up the environment, executing the fine-tuning script, and loading the resulting weights for inference.\n",
        "\n",
        "**Objective**: Our goal is to customize the SDXL model to generate images with specific styles or themes. To achieve this, we use LoRA, a parameter-efficient fine-tuning technique, making the process computationally feasible even on limited hardware.\n",
        "\n",
        "We'll start by setting up the environment using a custom shell script, build.sh, which handles the installation of required libraries and configurations. After that, we'll explain each parameter in the fine-tuning command to understand their roles in managing computational load and model performance.\n"
      ],
      "metadata": {
        "id": "zpGKdS0xgP3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "To simplify the environment setup, we have created a shell script (build.sh) that installs the necessary dependencies for SDXL fine-tuning. This includes cloning the diffusers repository, installing specific requirements for SDXL examples, and configuring tools to optimize computation.\n",
        "\n",
        "Simply run the following cell to execute the setup:"
      ],
      "metadata": {
        "id": "7lN7R0cYg6jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the setup script\n",
        "!bash build.sh"
      ],
      "metadata": {
        "id": "hey2MIxgddw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning with LoRA and `accelerate`\n",
        "\n",
        "Now that the environment is ready, let's fine-tune SDXL using LoRA. The following command leverages the `accelerate` library to handle efficient parallelism and optimization for large model training. Here’s a breakdown of each parameter:\n",
        "\n",
        "- `--pretrained_model_name_or_path`: The base model to fine-tune. Here, we're using `stabilityai/stable-diffusion-xl-base-1.0`.\n",
        "- `--pretrained_vae_model_name_or_path`: The pre-trained VAE model path for stable generation. We use `sdxl-vae-fp16-fix` for improved image quality.\n",
        "- `--dataset_name`: Specifies the dataset to use; in this case, a Pokémon captioning dataset.\n",
        "- `--dataloader_num_workers`: Set to 8 to increase data loading efficiency during training.\n",
        "- `--caption_column`: The column in the dataset that provides the captions for image generation.\n",
        "- `--train_batch_size`: Batch size of 5 helps balance memory load and training speed.\n",
        "- `--num_train_epochs`: We set this to 10, allowing sufficient training while controlling computational time.\n",
        "- `--learning_rate`: A low rate (1e-4) to prevent overfitting during fine-tuning.\n",
        "- `--lr_scheduler`: \"constant\" maintains a steady learning rate, simplifying training stability.\n",
        "- `--resolution`: Output image resolution. Here, we use 512 for quality and computational feasibility.\n",
        "- `--center_crop` & `--random_flip`: Basic data augmentations to improve model robustness.\n",
        "- `--output_dir`: Where fine-tuned weights will be saved.\n",
        "- `--validation_prompt`: Used to periodically check the model's output during training.\n",
        "- `--checkpointing_steps`: Save model checkpoints every 500 steps.\n",
        "- `--gradient_checkpointing` & `--gradient_accumulation_steps`: Techniques to handle large model gradients without overwhelming memory.\n",
        "- `--mixed_precision=\"fp16\"`: 16-bit floating point precision to reduce memory usage.\n",
        "- `--use_8bit_adam`: Optimizes computation with 8-bit Adam optimizer, reducing resource needs.\n",
        "- `--seed`: Setting for reproducibility.\n",
        "\n",
        "Run the following cell to start fine-tuning:\n"
      ],
      "metadata": {
        "id": "AbyvhH428IRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --config_file config.yaml diffusers/examples/text_to_image/train_text_to_image_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
        "  --dataset_name=\"svjack/pokemon-blip-captions-en-zh\" \\\n",
        "  --dataloader_num_workers=8 \\\n",
        "  --caption_column=\"en_text\" \\\n",
        "  --train_batch_size=5 \\\n",
        "  --num_train_epochs=10 \\\n",
        "  --learning_rate=1e-04 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --resolution=512 \\\n",
        "  --center_crop \\\n",
        "  --random_flip \\\n",
        "  --output_dir=\"sdxl-lora-weights\" \\\n",
        "  --validation_prompt=\"A cat in the forest.\" \\\n",
        "  --num_validation_images=5 \\\n",
        "  --checkpointing_steps=500 \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --seed=42\n"
      ],
      "metadata": {
        "id": "TizXnaTahUvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading LoRA Weights and Running Inference\n",
        "\n",
        "Once fine-tuning is complete, we can load the LoRA weights into the Stable Diffusion XL pipeline and run inference. The `diffusers` library provides a simple way to do this with the `StableDiffusionXLPipeline`, which allows us to leverage the fine-tuned model for custom image generation.\n",
        "\n",
        "The following code snippet loads the weights and generates an image based on a prompt. Two key parameters in this process are num_inference_steps and guidance_scale:\n",
        "\n",
        "* **num_inference_steps**: This parameter controls how many steps the model takes to generate the image. Higher values typically lead to more detailed images as the model has more iterations to refine the output. Here, we've set it to 100 to balance image quality with processing time.\n",
        "\n",
        "* **guidance_scale**: This parameter influences how closely the generated image follows the prompt. A higher guidance scale means the model will adhere more strictly to the prompt details, though excessively high values can sometimes affect image coherence. In this case, a guidance scale of 10 helps ensure the image aligns well with the prompt while maintaining visual quality.\n"
      ],
      "metadata": {
        "id": "L3rBoRAsiO-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "\n",
        "# Path where the LoRA weights are saved\n",
        "model_path = \"sdxl-lora-weights\"\n",
        "\n",
        "# Load the Stable Diffusion XL pipeline and set precision\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe.to(\"cuda\")  # Use GPU for faster inference\n",
        "\n",
        "# Load the fine-tuned LoRA weights\n",
        "pipe.load_lora_weights(model_path)\n",
        "\n",
        "# Generate an image with the fine-tuned model\n",
        "image = pipe(\n",
        "    prompt=\"A cat in the forest.\",\n",
        "    num_inference_steps=100,\n",
        "    guidance_scale=10\n",
        ").images[0]\n",
        "\n",
        "# Display the generated image\n",
        "image"
      ],
      "metadata": {
        "id": "wGiD7WoDiXf7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}