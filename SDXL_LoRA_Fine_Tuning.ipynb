{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOxW7dxDI/F/9JwoPub4deW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidricardocr/sdxl-lora-fine-tuning/blob/main/SDXL_LoRA_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stable Diffusion XL LoRA Fine-Tuning Guide"
      ],
      "metadata": {
        "id": "ISIFDlyXddJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this guide on fine-tuning Stable Diffusion XL (SDXL) with **LoRA (Low-Rank Adaptation)**. In this notebook, we will walk through setting up the environment, executing the fine-tuning script, and loading the resulting weights for inference.\n",
        "\n",
        "**Objective**: Our goal is to customize the SDXL model to generate images with specific styles or themes. To achieve this, we use LoRA, a parameter-efficient fine-tuning technique, making the process computationally feasible even on limited hardware.\n",
        "\n",
        "We'll start by setting up the environment using a custom shell script, build.sh, which handles the installation of required libraries and configurations. After that, we'll explain each parameter in the fine-tuning command to understand their roles in managing computational load and model performance.\n"
      ],
      "metadata": {
        "id": "zpGKdS0xgP3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "To simplify the environment setup, we have created a shell script (build.sh) that installs the necessary dependencies for SDXL fine-tuning. This includes cloning the diffusers repository, installing specific requirements for SDXL examples, and configuring tools to optimize computation.\n",
        "\n",
        "Simply run the following cell to execute the setup:"
      ],
      "metadata": {
        "id": "7lN7R0cYg6jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the setup script\n",
        "!bash build.sh"
      ],
      "metadata": {
        "id": "hey2MIxgddw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning with LoRA and `accelerate`\n",
        "\n",
        "Now that the environment is ready, let's fine-tune SDXL using LoRA. The following command leverages the `accelerate` library to handle efficient parallelism and optimization for large model training. Here’s a breakdown of each parameter:\n",
        "\n",
        "- `--pretrained_model_name_or_path`: The base model to fine-tune. Here, we're using `stabilityai/stable-diffusion-xl-base-1.0`.\n",
        "- `--pretrained_vae_model_name_or_path`: The pre-trained VAE model path for stable generation. We use `sdxl-vae-fp16-fix` for improved image quality.\n",
        "- `--dataset_name`: Specifies the dataset to use; in this case, a Pokémon captioning dataset.\n",
        "- `--dataloader_num_workers`: Set to 8 to increase data loading efficiency during training.\n",
        "- `--caption_column`: The column in the dataset that provides the captions for image generation.\n",
        "- `--train_batch_size`: Batch size of 5 helps balance memory load and training speed.\n",
        "- `--num_train_epochs`: We set this to 10, allowing sufficient training while controlling computational time.\n",
        "- `--learning_rate`: A low rate (1e-4) to prevent overfitting during fine-tuning.\n",
        "- `--lr_scheduler`: \"constant\" maintains a steady learning rate, simplifying training stability.\n",
        "- `--resolution`: Output image resolution. Here, we use 512 for quality and computational feasibility.\n",
        "- `--center_crop` & `--random_flip`: Basic data augmentations to improve model robustness.\n",
        "- `--output_dir`: Where fine-tuned weights will be saved.\n",
        "- `--validation_prompt`: Used to periodically check the model's output during training.\n",
        "- `--checkpointing_steps`: Save model checkpoints every 500 steps.\n",
        "- `--gradient_checkpointing` & `--gradient_accumulation_steps`: Techniques to handle large model gradients without overwhelming memory.\n",
        "- `--mixed_precision=\"fp16\"`: 16-bit floating point precision to reduce memory usage.\n",
        "- `--use_8bit_adam`: Optimizes computation with 8-bit Adam optimizer, reducing resource needs.\n",
        "- `--seed`: Setting for reproducibility.\n",
        "\n",
        "Run the following cell to start fine-tuning:\n"
      ],
      "metadata": {
        "id": "AbyvhH428IRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch --config_file config.yaml diffusers/examples/text_to_image/train_text_to_image_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
        "  --dataset_name=\"svjack/pokemon-blip-captions-en-zh\" \\\n",
        "  --dataloader_num_workers=8 \\\n",
        "  --caption_column=\"en_text\" \\\n",
        "  --train_batch_size=5 \\\n",
        "  --num_train_epochs=10 \\\n",
        "  --learning_rate=1e-04 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --resolution=512 \\\n",
        "  --center_crop \\\n",
        "  --random_flip \\\n",
        "  --output_dir=\"sdxl-lora-weights\" \\\n",
        "  --validation_prompt=\"A cat in the forest.\" \\\n",
        "  --num_validation_images=5 \\\n",
        "  --checkpointing_steps=500 \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --seed=42\n"
      ],
      "metadata": {
        "id": "TizXnaTahUvx",
        "outputId": "06630abd-06aa-4e68-cf66-16f3bf5d4b0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-03 15:35:33.100993: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-03 15:35:33.117089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-03 15:35:33.139190: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-03 15:35:33.145739: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-03 15:35:33.161647: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-03 15:35:34.425302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "11/03/2024 15:35:36 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "tokenizer/tokenizer_config.json: 100% 737/737 [00:00<00:00, 5.45MB/s]\n",
            "tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 13.0MB/s]\n",
            "tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 2.43MB/s]\n",
            "tokenizer/special_tokens_map.json: 100% 472/472 [00:00<00:00, 3.96MB/s]\n",
            "tokenizer_2/tokenizer_config.json: 100% 725/725 [00:00<00:00, 6.18MB/s]\n",
            "tokenizer_2/special_tokens_map.json: 100% 460/460 [00:00<00:00, 3.91MB/s]\n",
            "text_encoder/config.json: 100% 565/565 [00:00<00:00, 4.76MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "text_encoder_2/config.json: 100% 575/575 [00:00<00:00, 4.95MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "scheduler/scheduler_config.json: 100% 479/479 [00:00<00:00, 2.95MB/s]\n",
            "{'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'variance_type', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
            "model.safetensors: 100% 492M/492M [00:02<00:00, 237MB/s]\n",
            "model.safetensors: 100% 2.78G/2.78G [00:13<00:00, 210MB/s]\n",
            "config.json: 100% 631/631 [00:00<00:00, 3.78MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 207MB/s]\n",
            "{'use_quant_conv', 'shift_factor', 'latents_std', 'mid_block_add_attention', 'use_post_quant_conv', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
            "unet/config.json: 100% 1.68k/1.68k [00:00<00:00, 11.9MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 10.3G/10.3G [00:48<00:00, 211MB/s]\n",
            "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
            "README.md: 100% 1.14k/1.14k [00:00<00:00, 7.52MB/s]\n",
            "dataset_infos.json: 100% 804/804 [00:00<00:00, 6.77MB/s]\n",
            "(…)-00000-of-00001-78e564002aa9c8f0.parquet: 100% 99.7M/99.7M [00:01<00:00, 83.0MB/s]\n",
            "Generating train split: 100% 833/833 [00:00<00:00, 1449.14 examples/s]\n",
            "11/03/2024 15:37:01 - INFO - __main__ - ***** Running training *****\n",
            "11/03/2024 15:37:01 - INFO - __main__ -   Num examples = 833\n",
            "11/03/2024 15:37:01 - INFO - __main__ -   Num Epochs = 10\n",
            "11/03/2024 15:37:01 - INFO - __main__ -   Instantaneous batch size per device = 5\n",
            "11/03/2024 15:37:01 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 20\n",
            "11/03/2024 15:37:01 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
            "11/03/2024 15:37:01 - INFO - __main__ -   Total optimization steps = 420\n",
            "Steps:  10% 42/420 [04:24<41:12,  6.54s/it, lr=0.0001, step_loss=0.0708]\n",
            "model_index.json: 100% 609/609 [00:00<00:00, 3.94MB/s]\n",
            "\n",
            "Fetching 11 files:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   0% 0.00/335M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 21.0M/335M [00:00<00:01, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 52.4M/335M [00:00<00:01, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 83.9M/335M [00:00<00:01, 215MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 115M/335M [00:00<00:00, 227MB/s] \u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 147M/335M [00:00<00:00, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 178M/335M [00:00<00:00, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  63% 210M/335M [00:00<00:00, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 241M/335M [00:01<00:00, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 273M/335M [00:01<00:00, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 304M/335M [00:01<00:00, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 230MB/s]\n",
            "\n",
            "Fetching 11 files: 100% 11/11 [00:01<00:00,  6.60it/s]\n",
            "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[A{'use_beta_sigmas', 'rescale_betas_zero_snr', 'timestep_type', 'sigma_min', 'sigma_max', 'final_sigmas_type', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:00<00:00, 47.87it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 40.56it/s]\n",
            "11/03/2024 15:41:28 - INFO - __main__ - Running validation... \n",
            " Generating 3 images with prompt: A cat in the forest..\n",
            "Steps:  20% 84/420 [10:06<31:36,  5.64s/it, lr=0.0001, step_loss=0.0341]{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[A{'use_beta_sigmas', 'rescale_betas_zero_snr', 'timestep_type', 'sigma_min', 'sigma_max', 'final_sigmas_type', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 51.71it/s]\n",
            "11/03/2024 15:47:07 - INFO - __main__ - Running validation... \n",
            " Generating 3 images with prompt: A cat in the forest..\n",
            "Steps:  30% 126/420 [15:43<28:11,  5.75s/it, lr=0.0001, step_loss=0.0663]{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[A{'use_beta_sigmas', 'rescale_betas_zero_snr', 'timestep_type', 'sigma_min', 'sigma_max', 'final_sigmas_type', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 52.16it/s]\n",
            "11/03/2024 15:52:44 - INFO - __main__ - Running validation... \n",
            " Generating 3 images with prompt: A cat in the forest..\n",
            "Steps:  33% 139/420 [18:25<30:11,  6.45s/it, lr=0.0001, step_loss=0.0755]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading LoRA Weights and Running Inference\n",
        "\n",
        "Once fine-tuning is complete, we can load the LoRA weights into the Stable Diffusion XL pipeline and run inference. The `diffusers` library provides a simple way to do this with the `StableDiffusionXLPipeline`, which allows us to leverage the fine-tuned model for custom image generation.\n",
        "\n",
        "The following code snippet loads the weights and generates an image based on a prompt. Two key parameters in this process are num_inference_steps and guidance_scale:\n",
        "\n",
        "* **num_inference_steps**: This parameter controls how many steps the model takes to generate the image. Higher values typically lead to more detailed images as the model has more iterations to refine the output. Here, we've set it to 100 to balance image quality with processing time.\n",
        "\n",
        "* **guidance_scale**: This parameter influences how closely the generated image follows the prompt. A higher guidance scale means the model will adhere more strictly to the prompt details, though excessively high values can sometimes affect image coherence. In this case, a guidance scale of 10 helps ensure the image aligns well with the prompt while maintaining visual quality.\n"
      ],
      "metadata": {
        "id": "L3rBoRAsiO-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "\n",
        "# Path where the LoRA weights are saved\n",
        "model_path = \"sdxl-lora-weights\"\n",
        "\n",
        "# Load the Stable Diffusion XL pipeline and set precision\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe.to(\"cuda\")  # Use GPU for faster inference\n",
        "\n",
        "# Load the fine-tuned LoRA weights\n",
        "pipe.load_lora_weights(model_path)\n",
        "\n",
        "# Generate an image with the fine-tuned model\n",
        "image = pipe(\n",
        "    prompt=\"A cat in the forest.\",\n",
        "    num_inference_steps=100,\n",
        "    guidance_scale=10\n",
        ").images[0]\n",
        "\n",
        "# Display the generated image\n",
        "image"
      ],
      "metadata": {
        "id": "wGiD7WoDiXf7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}